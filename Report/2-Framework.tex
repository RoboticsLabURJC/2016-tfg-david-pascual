\chapter{Framework}\label{ch:framework}
This chapter serves as a way to introduce the tools that have been employed during the development of this project. All of them are \textbf{open-source}. The transparency provided by the open-source platforms is a major advantage, because the software can be joined together and adapted to our specific applications, which are mainly written in \textbf{Python} \footnote{\url{https://www.python.org/}}.

\section{Keras}\label{sec:keras}
As stated by \textbf{Keras} documentation~\cite{chollet2015keras}: "Keras is a high-level \textbf{neural network library}, written in Python and capable of running on top of either TensorFlow or Theano". TensorFlow and Theano are open-source libraries for numerical computation optimized for GPU and CPU that Keras treats as its \textit{backends}. In this project, Keras is running on top of \textbf{Theano} \footnote{\url{http://deeplearning.net/software/theano/index.html}} optimized for CPU, but it's quite easy to switch from one backend to another.

In the following sections, the main elements that make up a neural network built with Keras are going to be analyzed, starting with the \textbf{\textit{model} object}, its core component.

\subsection{Models}\label{subsec:models}
Every neural network in Keras is defined as a \textbf{\textit{model}}. For those models which can be built as a stack of \textit{layers} (see Section~\ref{subsec:layers}), Keras provides the \textbf{\textit{.Sequential()} object}. An example of a sequential model built with Keras can be seen in the following chapter in Figure~\ref{fig:model}. It is also possible to build more complex models with multiple outputs and shared layers using the \textbf{Keras functional API}.

Sequential models have several methods, and the following ones are essential for the learning process:
\begin{description}
	\item[\textit{.compile()}] It configures the \textbf{learning process}. It's main arguments are:
	\begin{itemize}
		\item \textbf{\textit{loss}}: name of the \textbf{cost function} employed to check the difference between the predicted labels and the real ones. In this project, the \textbf{categorical cross-entropy}, also known as log loss, has been used. This function returns the cross-entropy between an approximating distribution $q$ and a true distribution $p$~\cite{theano} and it's defined as:
		\begin{equation}\label{eq:categorical_crossentropy}
		H(p,q)=-\Sigma_{x}p(x)\log(q(x))
		\end{equation}		
		Other loss functions such as mean squared error (MSE), mean absolute error and hinge are also provided by Keras.
		
		\item \textbf{\textit{optimizer}}: name of the optimizer that will update the weights values during training in order to minimize the loss function. The chosen algorithm for this task is \textbf{ADADELTA}. This optimizer is an extension of the \textbf{gradient descent} optimization method that has the particularity of adapting the learning rate during training with no need of manual tuning. According to the paper in which it is defined~\cite{DBLP:journals/corr/abs-1212-5701}, it follows the next algorithm:		
		\begin{minipage}{\linewidth}
		\begin{algorithm}[H]
			\caption{Computing ADADELTA update at time $t$}\label{adadelta}
		  	\begin{algorithmic}[1]
		  		\Require{Decay rate $\rho$, Constant $\epsilon$}
		  		\Require{Initial parameter $x_1$}
		  		\State Initialize accumulation variables $E[g^2]_0=0, E[\Delta x^2]_0 = 0$
		  		\For{$t=1:T$} \Comment{Loop over \# of updates}
		  		\State Compute gradient: $g_t$
		  		\State Accumulate gradient: $E[g^2]_t=\rho E[g^2]_{t-1}+(1-\rho)g_t^2$
		  		\State Compute update: $\Delta x_t=-\frac{\mathrm{RMS}[\Delta x]_{t-1}}{\mathrm{RMS}[g]_t}g_t$
		  		\State Accumulate updates: $E[\Delta x^2]_t=\rho E[\Delta x^2]_{t-1}+(1-\rho)\Delta x_t^2$
		  		\State Apply update: $x_{t+1}=x_t+\Delta x_t$
				\EndFor
		  		\State \textbf{end for}
		  	\end{algorithmic}
		\end{algorithm}
		\end{minipage}\\
				
		Other optimization methods such as Adagrad, Adamax and Adam are also available.
		
		\item \textbf{\textit{metrics}}: name of the functions that must be evaluated during training and testing. The only one that is going to be computed with Keras through this project, besides the loss function, which is automatically computed, is \textbf{accuracy}. It is defined as the proportion of examples for which the model produces the correct output~\cite{Goodfellow-et-al-2016}.			
		Other measurements about the performance of the model are obtained with the \textbf{Scikit-learn} library (see Section~\ref{sec:sklearn}).
	\end{itemize}
\end{description}

\begin{description}
	\item[\textit{.fit()}] It trains the model. The following arguments are required:
	\begin{itemize}
		\item \textbf{\textit{x}, \textit{y}}: training samples and labels. They must be defined as \textbf{Numpy arrays}\footnote{\url{http://www.numpy.org/}}.
		
		\item \textbf{\textit{batch\_size}}: number of samples that are evaluated before updating the weights. It defaults to 32.
		
		\item \textbf{\textit{epochs}}: number of iterations over the whole dataset that are going to be executed. It defaults to 10.
		
		\item \textbf{\textit{callbacks}}: list of callbacks (see Section~\ref{subsec:callbacks}) that are going to be applied during training. It defaults to \textit{None}.
		
		\item \textbf{\textit{validation\_split} or \textit{validation\_data}}: in Keras, there are two alternatives to provide a validation dataset. On one hand, it is possible to pass the validation data as a Numpy array to the \textit{validation\_data} argument. On the other hand, a fraction of the training samples can be set as validation data through the \textit{validation\_split} argument. It's important to note that this new validation data won't be used for training anymore. \textit{validation\_data} and  \textit{validation\_split} arguments are mutually exclusive, so just one of them can be used.
		
		\item \textbf{\textit{shuffle}}: a boolean that determines whether to shuffle training data or not. 
	\end{itemize}
\end{description}

\begin{description}
	\item[\textit{.evaluate()}] It takes a set of samples and labels and evaluates the \textbf{model performance}, returning a list of the \textit{metrics} previously defined.
\end{description}

\begin{description}
	\item[\textit{.predict()}] It takes a sample and returns the label predicted by the model.
\end{description}

\begin{description}
	\item[\textit{.save()}] It stores the model into a \textbf{\gls{hdf5} file} (see Section~\ref{sec:hdf}), which will contain the weights, architecture and training configuration of the model.
\end{description}

\begin{description}
	\item[\textit{.load\_model()}] It loads a model from a \textbf{\gls{hdf5} file}.
\end{description}

\subsection{Layers}\label{subsec:layers}
As it has been said before, the models are usually built as a \textbf{stack of layers}. These layers are added to the model using the \textbf{\textit{.add()} method}, inside of which the kind of layer is declared and its particular parameters are set. Several kinds of layers are available, but only the ones that have been used in this project are going to be described.
\begin{description}
	\item[Convolutional layer] This particular layer is the one that turns the neural network into a \textbf{\gls{cnn}}. It is formed by a fixed number of \textbf{filters/kernels} with a fixed size. These filters are convolved along the input image, generating each one a \textbf{feature or activation map} which will tell us to what extent the feature learned by that particular filter is present in the input image~\cite{cs231n}. It's important to note that the depth of the filter will be equal to the number of channels of the input, which implies that each filter will generate just one activation map, instead of generating one for each channel. 
	
	Keras provides different kinds of convolutional layers depending on the input dimensions: \textit{Conv1D}, \textit{Conv2D} and \textit{Conv3D}. These are the main arguments required by Keras to define a convolutional layer:
	\begin{itemize}
		\item \textbf{\textit{filters}}: number of filters.
		
		\item \textbf{\textit{kernel\_size}}: width and height of the filters.
		
		\item \textbf{\textit{strides}}: how many pixels the filter must be shifted before applying the next convolution. It defaults to 1.
		
		\item \textbf{\textit{padding}}: it can be \textit{valid} or \textit{same}. If \textit{valid} mode is set, no padding is applied, resulting in a reduced output. However, if \textit{same} mode is set, the input will be padded with zeros in order to produce an output that preserves the input size. It defaults to \textit{valid}.
	\end{itemize}

	Figure~\ref{fig:convlayer} shows how the convolutional layers work. In Figure~\ref{fig:conv_a}, the filter $w_0$ (3x3x3) is convolved with the input image (5x5x3). As padding is set to 1 pixel around the input and the stride is equal to 2, the operation will return a 3x3 activation map. The same procedure is followed in Figure~\ref{fig:conv_b} with the filter $w_1$. It generates another 3x3 activation map, ending up with a 3x3x2 output. These images have been extracted from~\cite{cs231n}
	
	\begin{figure}
		\centering
		\begin{subfigure}{0.7\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{figures/convlayer_anime1big.png}
			\caption{}\label{fig:conv_a}
		\end{subfigure}
		\begin{subfigure}{0.7\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{figures/convlayer_anime2big.png}
			\caption{}\label{fig:conv_b}
		\end{subfigure}
		\caption{Convolutional layer.}
		\label{fig:convlayer}
	\end{figure}
	
\end{description}

\begin{description}
	\item[{Pooling layer}] It shifts a window of a certain size along the input image applying an operation (mean or maximum) that will return a \textbf{\textit{downsampled} version} of it, reducing the computational cost and avoiding overfitting~\cite{Scherer2010Evaluation}. Figure~\ref{fig:pooling} shows how the pooling operation is applied.

	\begin{figure}
		\centering
		\includegraphics[width=10cm, keepaspectratio]{figures/pooling.png}
		\caption{Example of a max. pooling operation.}
		\label{fig:pooling}
	\end{figure}
	
	Depending on the dimensions of the input and the operation applied, Keras provides several pooling layers: \textit{MaxPooling1D}, \textit{MaxPooling2D}, \textit{MaxPooling3D}, \textit{AveragePooling1D}... The main arguments required by Keras to define these layers are:
	\begin{itemize}
		\item \textbf{\textit{pool\_size}}: size of the window that is shifted along the input. It can also be interpreted as the factor by which the input is going to be \textit{downsampled}.
		\item \textbf{\textit{strides}}: how many pixels the window must be shifted before applying the next operation.
	\end{itemize}
\end{description}

\begin{description}
	\item[Dense layer] Fully-connected layers in Keras are defined as \textit{Dense layers}. In a \textbf{fully-connected layer}, every neuron is connected to every activation (i.e. output) of the previous one~\cite{cs231n}. The main argument of this layer is:
	\begin{itemize}
		\item \textbf{\textit{units}}: number of neurons.
	\end{itemize} 
\end{description}

\begin{description}
	\item[Activation layers] In Keras models, an activation function can be declared as a layer itself or as an argument within the \textit{.add()} method of the previous layer. Keras provides several \textbf{activation functions}, such as sigmoid, linear, \gls{relu} and softmax. These are the ones that have been used during the development of this project:
	\begin{itemize}
		\item \textbf{\gls{relu}}: this activation function introduces \textbf{non-linearity} right after each convolutional layer, allowing the \gls{cnn} to learn more complex features. It's defined as:
		\begin{equation}\label{eq:relu}
		g(z)=\max(0,z)
		\end{equation}
		
		\item \textbf{Softmax}: this activation function is very useful when is placed after the \textbf{output layer} of classification tasks. It takes a vector of real values $z$ and returns a new vector of real values in the range [0,1]. The $N$ elements of the output vector can be considered \textbf{probabilities} because the softmax function ensures that they sum up to 1. It is defined as follows:
		\begin{equation}\label{eq:SoftMax}
		\mathrm{softmax}(z)_i=\frac{\exp(z_i)}{\Sigma_{j}{\exp(z_j)}} \quad \mathrm{for} \ j=1, ...,N
		\end{equation}
	\end{itemize}
	These equations and definitions have been extracted from~\cite{Goodfellow-et-al-2016}. Figure~\ref{fig:activations} shows these activation functions plotted in the interval $[-1,1]$.

	\begin{figure}
		\centering
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{figures/relu.png}
			\caption{\gls{relu} activation function.}\label{fig:relu}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{figures/softmax.png}
			\caption{Softmax activation function.}\label{fig:softmax}
		\end{subfigure}
		\caption[Activation functions.]{}
		\label{fig:activations}
	\end{figure}
	
\end{description}

\begin{description}
	\item[Flatten layer] It \textit{flattens} the input. For instance, it converts the activation maps returned by the convolutional layers into a \textbf{vector of weights} before being connected to a dense layer. It takes no arguments.
\end{description}

\begin{description}
	\item[Dropout layer] It's considered a \textbf{regularization layer}, because its main purpose is to avoid overfitting. Dropout is a technique that randomly \textbf{\textit{switches-off}} a fraction of hidden units during training~\cite{srivastava2014dropout}. It can also be understood as a technique that "trains an ensemble consisting of all subnetworks that can be structured by removing non-output units from an underlying base network"~\cite{Goodfellow-et-al-2016}, as it can be seen in Figure~\ref{fig:dropout}.

	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth, keepaspectratio]{figures/dropout.png}
		\caption{Subnetworks generated when using dropout.}
		\label{fig:dropout}
	\end{figure}	

	 This layer, as other regularization layers (i.e. GaussianNoise layer), is only active during training. It's main argument is:
	\begin{itemize}
		\item \textbf{\textit{rate}}: fraction of units that must be dropped.
	\end{itemize}
	
\end{description}

\subsection{Callbacks} \label{subsec:callbacks}
As defined by Keras documentation~\cite{chollet2015keras}, \textbf{callbacks} are a set of functions which are applied at given stages while the model is being trained. They can be used to take a look at the state of the model during training. The built-in callbacks that have been used for this project are:
\begin{itemize}
	\item \textbf{\textit{.History()}}: it is automatically applied to every Keras model and is returned by the \textit{.fit()} method. After each epoch, this callback evaluates the declared \textit{metrics} with the validation dataset and saves the results.
	
	\item \textbf{\textit{.EarlyStopping()}}: it monitors the value of a given function and forces the model to stop training when that function has stopped improving. It has a \textbf{\textit{patience}} argument which determines how many epochs in a row without improving must be tolerated before the model quits training. Setting up an appropriate \textbf{stopping criteria} may prevent the model from overfitting.
	
	\item \textbf{\textit{.ModelCheckpoint()}}: it saves the model and its weights after each epoch. It can be configured to overwrite the model only if a certain \textit{metric} has improved with respect to the previous best result, saving the best \textit{version} of it.
\end{itemize}

Additionally, Keras provides the \textit{Callback} base class that can be used to build \textbf{user-defined callbacks}.

\subsection{Image Preprocessing}\label{subsec:utils}
\textbf{Image preprocessing} is a key factor in every computer vision application. Specifically, in machine learning, besides adapting the images and extracting features before training that can improve the model performance (i.e. edge detection), it can be used to avoid \textbf{overfitting} through data augmentation. \textbf{Data augmentation}~\cite{DBLP:journals/corr/WongGSM16} consists in taking the samples that the dataset already contains and applying transformations to them, generating new samples that may be closer to real world and, in any case, enlarging the dataset with new data.

This functionality is included in Keras thanks to the \textbf{\textit{.ImageDataGenerator()} method}. It returns a batch generator which randomly applies the desired transformations to random samples of the dataset provided by the user. Built-in transformations like rotation, shifting and zooming, are passed as arguments to the aforementioned method. Additionally, it's possible to build a user-defined function and pass it as an argument as well. The dataset and the batch size are defined through the \textbf{\textit{.flow()} method}. During training, the generator will loop until the number of samples per epoch and the number of epochs set by the user are satisfied.

\subsection{Utils}
Keras include a module for multiple supplementary tasks called \textbf{\textit{Utils}}. The most important functionality for the project provided by this module is the \textbf{\textit{.HDF5Matrix()} method}. It reads the \textbf{\gls{hdf5} datasets} (see Section~\ref{sec:hdf}), which are going to be used as inputs to the neural networks.

\section{JdeRobot}\label{sec:jderobot}
\textbf{JdeRobot} is an open source middleware for robotics and computer vision~\cite{jderobot}. It has been designed to simplify the software development within these fields. It's mostly written in C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++} language and it's structured like a collection of components (tools and drivers) that communicate to each other through \textbf{ICE interfaces} \footnote{\url{https://zeroc.com/products/ice}}. It is also compatible with \textbf{ROS} \footnote{\url{http://www.ros.org/}}, which allows the interoperation of ROS nodes and JdeRobot components. This flexibility makes it very useful for our application.
Its \textbf{\textit{cameraserver} driver} is going to be employed to capture images from different video sources.

\subsection*{\textit{cameraserver}}
According to JdeRobot documentation~\cite{jderobot}, this driver can serve both real cameras and video files. It communicates with other components thanks to the \textbf{\textit{Camera} interface}.

In order to use \textit{cameraserver}, its \textbf{configuration file} has to be properly set. These are the parameters that must be specified:
\begin{itemize}
	\item The \textbf{network address} where the server is going to be listening.
	
	\item Parameters related with the \textbf{video stream}: URI, frame rate, image size and format.
\end{itemize}

\section{DroidCam}\label{sec:droidcam}
On one hand, \textbf{DroidCam} is an application for Android which serves the images captured with a \textbf{smartphone camera}~\cite{droidcam}. On the other hand, it is a client for Linux which receives the video stream served by Android and makes it accessible for the computer as a \textbf{v4l2 \footnote{\url{https://www.linuxtv.org/wiki/index.php/Main_Page}} device driver}. The Linux client can be connected to the phone camera over a USB cable or a WiFi network and allows the user to control camera flash, auto-focus and zoom. DroidCam provides the address at which the Linux client must be listening to receive the images. Besides that, it provides a URL that can be used to access the video stream from any browser. 

An example of usage can be seen in Figure~\ref{fig:droidcam}. First, the Android app is opened. It shows the address where the video will be served (see Figure~\ref{fig:droidpre}). Then, the address is set in the Linux client (see Figure~\ref{fig:droidlinux}). Finally, when the \textit{Connect} button is pressed, the connection is established (see Figure~\ref{fig:droidpost})
\begin{figure}
	\centering
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/droidcampre.png}
		\caption{Android server.}
		\label{fig:droidpre}
	\end{subfigure}%
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/droidcamlinux.png}
		\caption{Linux Client.}
		\label{fig:droidlinux}
	\end{subfigure}%
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/droidcampost.png}
		\caption{Connection established.}
		\label{fig:droidpost}
	\end{subfigure}
	\caption[DroidCam usage.]{}
	\label{fig:droidcam}
\end{figure}

\section{HDF5}\label{sec:hdf}
During the development of this project, huge amounts of data have been processed. That's why an efficient way of reading and saving this data has been an important point. Keras employs the \textbf{\gls{hdf5} file format} to save models and read datasets.

According to \gls{hdf5} documentation~\cite{hdf5}, it is a \textbf{hierarchical data format} designed for high volumes of data with complex relationships. While relational databases employ tables to store data (e.g. SQL), \gls{hdf5} supports \textbf{n-dimensional datasets} and each element in the dataset may be as complex as needed.

In order to deal with \gls{hdf5} files, the \textbf{h5py} \footnote{\url{http://www.h5py.org/}} library for Python has been employed.

\section{Scikit-learn}\label{sec:sklearn}
\textbf{Scikit-learn} is a machine learning library that includes a wide variety of algorithms for clustering, regression and classification~\cite{scikit-learn}. It can be used at every stage of the machine learning workflow: preprocessing, training, model selection and evaluation.

Scikit-learn functions have been used to evaluate the neural networks developed with Keras. Using a tool that is \textbf{independent from Keras} enables the comparison of the results achieved by different neural network libraries (e.g. Keras and Caffe). These are the evaluation parameters which have been employed in this project (equations and definitions obtained from~\cite{scikit-doc}):
\begin{itemize}
	\item \textbf{Precision}: ability of the classifier not to label as positive a sample that is negative.
	\begin{equation}\label{eq:precision}
	\textrm{precision}=\frac{true_{positives}}{true_{positives}+false_{positives}}
	\end{equation}
	
	\item \textbf{Recall}: ability of the classifier to find all the positive samples.
	\begin{equation}\label{eq:recall}
	\textrm{recall}=\frac{true_{positives}}{true_{positives}+false_{negatives}}
	\end{equation}
	
	\item \textbf{Confusion matrix}: a matrix where the element $i,j$ represents the number of samples that belongs to the group $i$ but has been classified as belonging to group $j$. True predictions can be found in the diagonal of the matrix, where $i=j$. An example of a confusion matrix constructed with Scikit-learn and displayed with Octave (see Section~\ref{sec:octave}) can be found in Figure~\ref{fig:conf_mat}.
\end{itemize}

Besides the functions that have just been mentioned, \textbf{accuracy} and \textbf{log loss} have also been used and they're defined as in Section~\ref{subsec:models}.

\section{Octave} \label{sec:octave}
\textbf{GNU Octave}~\cite{octave} is a scientific programming language compatible with \textbf{Matlab}. It provides powerful tools for plotting, which have been used to visualize the data collected with Scikit-learn about the performance of the models. An example of Octave usage can be seen in Figure~\ref{fig:conf_mat}.
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth, keepaspectratio]{figures/conf_mat.png}
	\caption{Example of a confusion matrix visualization using Octave.}
	\label{fig:conf_mat}
\end{figure}
\chapter{Benchmark}\label{ch:benchmark}
The model described in Section~\ref{sec:understanding} can be improved with different architectures and regularization methods. Besides that, training the model with new datasets can also lead to better results. In order to \textbf{compare the performance} of these new models, a \textbf{benchmark} has been developed. In this chapter, the datasets employed to train the models, as well as the tools developed to measure and visualize their performance will be described.

\section{Datasets}\label{sec:datasets}
The digit classifier component is possible thanks to the data provided by the \textbf{MNIST database} of handwritten digits. In the following sections, the pros and cons of using this database and some alternatives will be discussed.

\subsection{Original dataset}\label{subsec:MNIST}
\textbf{MNIST (Modified National Institute of Standards and Technology database)} is a database of \textbf{handwritten digits} formed by a training set, which contains 60000 samples, and a test set, containing 10000 samples~\cite{lecun-mnisthandwrittendigit-2010}. It's a \textit{remixed} and reduced version of the original \textbf{NIST datasets} \footnote{\url{https://www.nist.gov/srd/nist-special-database-19}}. MNIST is a well-known benchmark for all kinds of machine learning algorithms.

As may be seen in Figure~\ref{fig:mnist}, each sample of the MNIST database is a 28x28 pixels \textbf{grayscale image} that contains a size-normalized and centered digit. While it may be useful for testing machine learning algorithms, it's not enough to train a model that aims to solve a \textbf{real-world task}, because the images are almost noiseless and share similar orientation, position, size and intensity levels.
\begin{figure}
	\centering
	\includegraphics[width=12cm, keepaspectratio]{figures/mnist.png}
	\caption{Samples extracted from the MNIST database.}
	\label{fig:mnist}
\end{figure}

\subsection{Gradient images}\label{subsec:edge}
The first issue with MNIST database that must be addressed is that the grayscale images that it contains share similar intensity levels: a white digit over a black background. In real world, the digits can be found written in several colors over different backgrounds and the datasets must resemble every possible combination. In order to achieve that generalization, the \textbf{gradient of the images} has been calculated. The resultant samples are less dependent from the light and color conditions than the original ones, forcing the neural network to focus in the shape of the digits to classify them.

According to the study carried out by Nuria Oyaga \footnote{\url{http://jderobot.org/Noyaga-tfg\#Testing\_Neural\_Network}}, the operator that leads to better results is the \textbf{Sobel filter}. This operator approximates the gradient of an image function~\cite{sonka1999image}, convolving the image with the following kernels to highlight horizontal and vertical edges, respectively:  
\begin{equation}\label{eq:sobel}
h_x = 
\begin{bmatrix}
1 & 2 & 1\\
0 & 0 & 0\\
-1 & -2 & -1
\end{bmatrix}
,\quad
h_y = 
\begin{bmatrix}
-1 & 0 & 1\\
-2 & 0 & 2\\
-1 & 0 & 1
\end{bmatrix}
\end{equation}
The absolute values of the resultant images, $x$ and $y$, are then added, obtaining the gradient image.

\subsection{Data augmentation}
The second problem that has been detected with MNIST is that the images are \textbf{noiseless} and the digits are always centered with a scale and a rotation angle that are almost \textbf{invariant}. However, the digit classifier has to deal with noisy images that can be randomly scaled, translated and/or rotated. In order to get a database with images that look like the ones that our application is going to work with, the MNIST database must be \textbf{augmented}.

Two alternatives have been considered to solve this problem: real-time data augmentation provided by Keras and generating our own database.

\subsubsection{Real-time data augmentation with Keras}
Thanks to the \textit{.ImageDataGenerator()} method provided by Keras (see Section~\ref{subsec:utils}), the MNIST dataset can be augmented in \textbf{real-time} during training. In order to cover most of the real cases, random rotation, translation and zooming has been applied to generate new samples. In addition to that, a Sobel filtering was also applied through a user-defined function. The samples generated by the following code \footnote{\url{https://git.io/vH0qz}} can be seen in Figure~\ref{fig:aug_keras}.

\begin{lstlisting}
datagen = imkeras.ImageDataGenerator(
	                  zoom_range=0.2, rotation_range=20, width_shift_range=0.2, 
	                  height_shift_range=0.2, fill_mode='constant', cval=0,
	                  preprocessing_function=self.sobelEdges)
...	              
generator = datagen.flow(x, y, batch\_size=batch\_size)
\end{lstlisting}

\begin{figure}
	\centering
	\includegraphics[width=12cm, keepaspectratio]{figures/aug_keras.png}
	\caption{Samples generated with Keras from MNIST database.}
	\label{fig:aug_keras}
\end{figure}

Besides these transformations, it's also necessary to simulate the \textbf{noise} that may be present in real images. Keras generator doesn't support the addition of noise. For this purpose, Keras includes noise layers such as the \textbf{GaussianNoise layer}, which adds Gaussian noise with a standard deviation distribution defined by the user. It's important to note that Keras treat noise layers as regularization methods that are only active during training time to avoid overfitting. In order to add noise to the generated samples, a GaussianNoise layer was established as the \textbf{input layer} of the model.

\subsubsection{Handmade augmented datasets}\label{subsec:handmade}
The alternative to real-time data augmentation with Keras is building \textbf{our own datasets} applying the previously mentioned transformations to the images. My mate Nuria Oyaga has build 5 new databases with two sets each one: training and validation \footnote{\url{http://jderobot.org/Noyaga-tfg\#Comparing_Neural_Network}}. These are the new databases: 
\begin{itemize}
	\item \textbf{Sobel}: MNIST database after applying the Sobel filter to every image. 48000 samples for traning and 12000 samples for validation. 
	\item \textbf{0-1}: Same size than Sobel database. One transformed image per every Sobel database image. Sobel database images are replaced by the the transformed ones. 48000 samples for traning and 12000 samples for validation. 
	\item \textbf{1-1}: Double size than Sobel database. One transformed image per every Sobel database image. Both Sobel database images and the transformed images are included in the 1-1 database. 96000 samples for training and 24000 samples for validation. 
	\item \textbf{0-6}: Six times the size of Sobel database. Six transformed images per every Sobel database image. Sobel database images are replaced by the transformed ones. 288000 samples for training and 72000 samples for validation. 
	\item \textbf{1-6}: Seven times the size of Sobel database. Six transformed images per every Sobel database image. Both Sobel database images and the transformed images are included in the 1-6 database. 336000 samples for training and 84000 samples for validation. 
\end{itemize}

Besides that, the test dataset of the MNIST database (10000 samples) has been converted into a \textbf{1-6 test dataset} (70000 samples).

In Figure~\ref{fig:aug_nuria}, the first samples of every handmade dataset can be seen.

\begin{figure}
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/Sobel.png}
		\caption{Sobel dataset.}\label{fig:sobel}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/0-1.png}
		\caption{0-1 dataset.}\label{fig:0-1}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/1-1.png}
		\caption{1-1 dataset.}\label{fig:1-1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/0-6.png}
		\caption{0-6 dataset.}\label{fig:0-6}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/1-6.png}
		\caption{1-6 dataset.}\label{fig:1-6}
	\end{subfigure}
	\caption[First samples of handmade datasets.]{}
	\label{fig:aug_nuria}
\end{figure}

\subsubsection{From LMDB to HDF5} \label{subsubsec:lmdb2hdf5}
These databases were initially built to feed a \textbf{Caffe}~\cite{jia2014caffe} neural network. That's why they were saved as \textbf{LMDB files} \footnote{\url{http://www.lmdb.tech/doc/}}. In order to make it easier to feed the Keras model, the LMDB databases have been converted into \textbf{HDF5 files} (see Section~\ref{sec:hdf}). For this conversion, the script \textbf{\textit{datasetconversion.py}} \footnote{\url{https://git.io/vHWTe}} has been written.

\begin{itemize}
	\item \textbf{Reading the LMDB database}. The LMDB library for Python \footnote{\url{https://lmdb.readthedocs.io/en/release/\#}} was employed to open the database, initialize a cursor and iterate over each key-value pair in the database. In addition, Google's Protocol Buffers \footnote{\url{https://developers.google.com/protocol-buffers/}}, a.k.a. Protobuf, was used to parse the data that was extracted from the database. "With protocol buffers, you write a \textit{.proto} description of the data structure you wish to store. From that, the protocol buffer compiler creates a class that implements automatic encoding and parsing of the protocol buffer data with an efficient binary format"~\cite{protobuf}. Here can be seen the \textit{.proto} file that defines the data structure used by Caffe to store the MNIST database, as obtained from~\cite{lmdb_tutorial}: 
	\begin{lstlisting}
	package datum;
	message Datum {
		optional int32 channels = 1;
		optional int32 height = 2;
		optional int32 width = 3;
		// the actual image data, in bytes
		optional bytes data = 4;
		optional int32 label = 5;
		// Optionally, the datum could also hold float data.
		repeated float float_data = 6;
		// If true data contains an encoded image that need to be decoded
		optional bool encoded = 7 [default = false];
	}
	\end{lstlisting}
	
	Thanks to the \textit{.proto} file, the compiler generates a Python module that contains the \textbf{Datum class}. Datum class provides the \textbf{\textit{.ParseFromString()} method}, which is employed to parse the image data. Here is the resulting code:
	\begin{lstlisting}
	# We initialize the cursor that we're going to use to access every
	# element in the dataset.
	lmdb_env = lmdb.open(sys.argv[1])
	lmdb_txn = lmdb_env.begin()
	lmdb_cursor = lmdb_txn.cursor()
	
	x = []
	y = []
	nb_samples = 0
	
	# Datum class deals with Google's protobuf data.
	datum = datum.Datum()
	
	if __name__ == '__main__':
		# We extract the samples and its class one by one.
		for key, value in lmdb_cursor:
		datum.ParseFromString(value)
		label = np.array(datum.label)
		data = np.array(bytearray(datum.data))
		im = data.reshape(datum.width, datum.height,
		datum.channels).astype("uint8")
		
		x.append(im)
		y.append(label)
		nb_samples += 1
		
		print("Extracted samples: " + str(nb_samples) + "\n")
		
		x = np.asarray(x)
		y = np.asarray(y)
	\end{lstlisting}
	
	\item \textbf{Writing the HDF5 files}. After extracting the data, it was stored in a HDF5 file. Thanks to the \textbf{h5py library} \footnote{\url{http://www.h5py.org/}} for Python, a HDF5 file with two \textit{datasets} (label and data) was created. The code can be seen in the following frame:	 
	\begin{lstlisting}
	f = h5py.File("../../Datasets/" + sys.argv[2] + ".h5", "w")
	
	# We store images.
	x_dset = f.create_dataset("data", (nb_samples, datum.width,
	datum.height, datum.channels), dtype="f")
	x_dset[:] = x
	
	# We store labels.
	y_dset = f.create_dataset("labels", (nb_samples,), dtype="i")
	y_dset[:] = y
	f.close()
	\end{lstlisting}
\end{itemize} 

\subsubsection{Conclusions}
After coding and testing both implementations for augmenting the database, it has been decided to go for the \textbf{handmade datasets}. While real-time data augmentation is really useful to avoid storing all the data that is needed for training, it makes it harder to take a look into what is being fed to the network and reproduce results. Also, in this particular case, we are interested in \textbf{compare the performance} of neural networks built with different libraries, so they must be trained with the same datasets.

\section{Measuring performance}\label{sec:measuring}
The performance of the models will be evaluated using the \textbf{\textit{CustomEvaluation} class} and the measurements calculated with this class will be visualized using \textbf{an Octave function}. In this section, both of them will be described.

\subsection{\textit{CustomEvaluation} class}
\textbf{\textit{CustomEvaluation} class} \footnote{\url{https://git.io/vHP47}} is totally \textbf{independent} from Keras. It calls functions that measure the performance of the model during \textbf{test and/or learning time} and saves them into a file which is compatible with Octave.
\begin{itemize}
	\item \textbf{Obtaining the measurements}. The user provides the \textbf{real labels} and the \textbf{probability distribution of the predicted ones}. Log loss, accuracy, precision, recall and a confusion matrix are computed. These functions are defined in Section~\ref{sec:sklearn} and Section~\ref{subsec:models}. Only log loss requires a probability distribution to be calculated. When calling the other functions, the predicted labels must be passed as an argument. The \textbf{predicted labels} are obtained as the indices of the maximum values in the probability distributions provided by the user.
	\item \textbf{Storing results}. \textit{CustomEvaluation} class stores the results in a \textbf{Python dictionary}. Additionally, it can store the \textbf{learning curves} if \textit{training} option is set. In the following section, the Keras callback employed to build the learning curves will be discussed.
	\item \textbf{Python-Octave \textit{translation}}. For this task, the \textbf{SciPy library} \footnote{\url{https://docs.scipy.org/doc/scipy-0.18.1/reference/index.html}} has been used. It provides the \textbf{\textit{.savemat()}} method that saves Python dictionaries into Matlab \textbf{\textit{.mat} files}, which are also compatible with Octave (see Section~\ref{sec:octave}).
\end{itemize}

Here's a usage example:
\begin{lstlisting}
if training == "n":
	measures = CustomEvaluation(y_test, y_proba, training)
else:
	train_loss = learning_curves.loss
	train_acc = learning_curves.accuracy
	val_loss = validation.history["val_loss"]
	val_acc = validation.history["val_acc"]
	results = CustomEvaluation(y_test, y_proba, training, train_loss,
	train_acc, val_loss, val_acc)

results_dict = results.dictionary()
results.log(results_dict)
\end{lstlisting}

\subsubsection{\textit{LearningCurves} callback} \label{subsubsec:learningcurves}
During training time, Keras automatically saves into a \textit{.History()} object (see Section~\ref{subsec:callbacks}) the \textbf{validation results} (loss and accuracy) obtained after every epoch. It's interesting to face these validation results with the ones obtained after every batch during training.

\textbf{\textit{LearningCurves}} \footnote{\url{https://git.io/vHP4N}} is a custom Keras callback that has been coded to save the accuracy and loss obtained \textbf{after each batch} into \textbf{Python lists}. The code below shows how it works:
\begin{lstlisting}
class LearningCurves(keras.callbacks.Callback):
''' LearningCurve class is a callback for Keras that saves accuracy
and loss after each batch.
'''    

def on_train_begin(self, logs={}):
	self.loss = []
	self.accuracy = []

def on_batch_end(self, batch, logs={}):
	self.loss.append(float(logs.get('loss')))
	self.accuracy.append(float(logs.get('acc')))
\end{lstlisting}

\subsection{Octave function}
Now that all the data has been collected, it has to be properly displayed. The function \textbf{\textit{benchmark.m}} \footnote{\url{https://git.io/vHPBt}} has been written to address this issue. It takes as its only argument the path to the \textit{.mat} file that has been generated with the \textit{CustomEvaluation} class and \textbf{plots the results} as they can be seen in Figure~\ref{fig:benchmark}. Additionally, it prints them to the \textbf{standard output}.

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/learning_curves.png}
		\caption{Learning curves.}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/prec_rec.png}
		\caption{Precision and recall.}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/conf_mat.png}
		\caption{Confusion matrix.}
	\end{subfigure}
	\caption[Example of usage of \textit{benchmark.m}.]{}
	\label{fig:benchmark}
\end{figure}

\section{Convolutional layers visualization}\label{sec:visualization}
\glspl{cnn} are well-known by their ability of learning \textbf{image features}. The weights of a convolutional layer are arranged like \textbf{a set of filters}, each of which learns to identify a certain visual feature~\cite{cs231n}. As the filter is convolved with the input image, it generates an \textbf{activation map} that will tell us how that particular filter reacts to that image. In other words, the activation map will tell us whether a certain feature is present in the image or not.

In order to understand how the Keras model is learning to classify the digits, the class \textbf{\textit{layer\_visualization.py}}~\footnote{\url{https://git.io/vH94o}} has been written. This class allows the user to display the filters that are learned in every convolutional layer of the model and their resulting activation maps.

\subsection{Filters}
Keras provides a list containing every layer object in the model within its attribute \textit{model.layers}. Layer objects properties can be accessed thanks to the \textit{layer.get\_config()} method. Since convolutional layers in Keras are named with the prefix \textit{conv2d}, we iterate the names of the layers looking for that prefix to find the convolutional layers, as it can be seen in the code below.

\begin{lstlisting}
for i, layer in enumerate(self.model.layers):
	if layer.get_config()["name"][:6] == "conv2d":
...
\end{lstlisting}

Once the convolutional layer has been found, accessing the filters is as easy as calling the \textit{layer.get\_weigths()} method. Besides that, the weights are reshaped to improve readability. The code that performs this process can be seen in the following frame:

\begin{lstlisting}
shape = layer.get_weights()[0].shape
weights = layer.get_weights()[0].reshape(shape[2], shape[0],
                                         shape[1], shape[3])
\end{lstlisting}

Finally, the filters are plotted with the \textit{plot\_data()} method, which has been written employing the \textbf{Matplotlib library}~\cite{Hunter:2007} for Python. Additionally, the shape and the maximum and minimum values of the weights are printed to the standard output. An example of how the filters are plotted can be seen in Figure~\ref{fig:filters}

\subsection{Activation maps}
The output of each convolutional layer is formed by as many \textbf{activation maps} as filters have the layer. In order to get the values of these activation maps, \textbf{truncated versions} of the original model are generated, as it can be seen in Figure~\ref{fig:truncated}. When a prediction is made with these truncated models, they output the activation maps that correspond to their last layer. In the following frame, the corresponding code can be seen.

\begin{lstlisting}
truncated = Model(inputs=self.model.inputs,
                  outputs=layer.output)
activations = truncated.predict(self.im)
\end{lstlisting}

\begin{figure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/1stconvarch.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/2ndconvarch.png}
		\caption{}
	\end{subfigure}
	\caption{Truncated versions of the model.}
	\label{fig:truncated}
\end{figure}

The activation maps are plotted using the same method than before, \textit{plot\_data()}, and some information about their shape and values is also printed to the standard output. An example of how the activation maps are plotted can be seen in Figure~\ref{fig:activation_maps}.
